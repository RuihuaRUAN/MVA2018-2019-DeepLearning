{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "        for i, word in enumerate(self.word2id.keys()):\n",
    "            self.word2id[word] = i\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        similarities = np.zeros(len(self.word2vec))\n",
    "        for idx, word in self.id2word.items():\n",
    "            similarities[idx] = self.score(w, word)\n",
    "        \n",
    "        k_nearest_idxs = np.argsort(similarities)[-K-1:-1][::-1]\n",
    "        \n",
    "        res = []\n",
    "        for idx in k_nearest_idxs:\n",
    "            res.append(self.id2word[idx])\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        # cosine similarity: np.dot  -  np.linalg.norm\n",
    "        embed1 = self.word2vec[w1]\n",
    "        embed2 = self.word2vec[w2]\n",
    "        similarity = self.cosine_similarity(embed1, embed2)\n",
    "        return similarity\n",
    "    \n",
    "    def cosine_similarity(self, v1, v2):\n",
    "        similarity = np.dot(v1.reshape((1, -1)), v2.reshape(-1, 1)) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "        return similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "cat dog [[0.67168367]]\n",
      "dog pet [[0.6842064]]\n",
      "dogs cats [[0.70743893]]\n",
      "paris france [[0.77751085]]\n",
      "germany berlin [[0.74202952]]\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']\n",
      "['france', 'Paris', 'london', 'berlin', 'tokyo']\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "    \n",
    "    def encode(self, sentences, idf=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        if len(sentences) == 0:\n",
    "            return\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                # mean of word vectors\n",
    "                embed = []\n",
    "                words = sent.split(' ')\n",
    "                for _, word in enumerate(words):\n",
    "                    if word in self.w2v.word2vec.keys():\n",
    "                        embed.append(self.w2v.word2vec[word])\n",
    "                    \n",
    "                if len(embed) == 0:\n",
    "                    embed = np.zeros((1, 300))\n",
    "                else:\n",
    "                    embed = np.vstack(embed)\n",
    "                sentemb.append(np.mean(embed, axis=0))\n",
    "            else:\n",
    "                # idf-weighted mean of word vectors\n",
    "                embed = []\n",
    "                words = sent.split(' ')\n",
    "                for _, word in enumerate(words):\n",
    "                    if word in self.w2v.word2vec.keys() and word in idf.keys():\n",
    "                        embed.append(self.w2v.word2vec[word] * idf[word])\n",
    "                    \n",
    "                if len(embed) == 0:\n",
    "                    embed = np.zeros((1, 300))\n",
    "                else:\n",
    "                    embed = np.vstack(embed)\n",
    "                sentemb.append(np.mean(embed, axis=0))\n",
    "        return np.vstack(sentemb)\n",
    "\n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = self.encode([s], idf)\n",
    "        similarities = np.zeros(keys.shape[0])\n",
    "        for i in range(keys.shape[0]):\n",
    "            key = keys[i]\n",
    "            similarities[i] = self.w2v.cosine_similarity(query, key)\n",
    "        \n",
    "        k_nearest_idxs = np.argsort(similarities)[-K-1:-1][::-1]\n",
    "        \n",
    "        for idx in k_nearest_idxs:\n",
    "            print(sentences[idx])\n",
    "        \n",
    "        return\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "        # cosine similarity: use   np.dot  and  np.linalg.norm\n",
    "        e1 = self.encode([s1], idf)\n",
    "        e2 = self.encode([s2], idf)\n",
    "        similarity = self.w2v.cosine_similarity(e1, e2)\n",
    "        print(similarity)\n",
    "        return\n",
    "    \n",
    "    def build_idf(self, sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {}\n",
    "        for sent in sentences:\n",
    "            words = sent.split(' ')\n",
    "            for w in set(words):\n",
    "                idf[w] = idf.get(w, 0) + 1\n",
    "                \n",
    "        for word in idf.keys():\n",
    "            idf[word] = max(1, np.log10(len(sentences) / (idf[word])))\n",
    "            \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 pretrained word vectors\n",
      "\n",
      "sentence embedding results without idf (BoV-mean)\n",
      "\n",
      "sentence 10 is: 1 smiling african american boy .\n",
      "The most similar sentences are: \n",
      "an african american man smiling .\n",
      "a little african american boy and girl looking up .\n",
      "an afican american woman standing behind two small african american children .\n",
      "an african american man is sitting .\n",
      "a girl in black hat holding an african american baby .\n",
      "\n",
      "sentence 7 is: 1 man singing and 1 man playing a saxophone in a concert .\n",
      "sentence 13 is: 10 people venture out to go crosscountry skiing .\n",
      "The score between sentence 7 and sentence 13 is: \n",
      "[[0.57262589]]\n",
      "\n",
      "sentence embedding results with idf (BoV-idf)\n",
      "\n",
      "an african american man smiling .\n",
      "an african american man is sitting .\n",
      "a little african american boy and girl looking up .\n",
      "an afican american woman standing behind two small african american children .\n",
      "a girl in black hat holding an african american baby .\n",
      "[[0.47514509]]\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=100000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with io.open(os.path.join(PATH_TO_DATA, 'sentences.txt'), encoding='utf-8') as f:\n",
    "    for _, line in enumerate(f):\n",
    "        sentences.append(line.strip())\n",
    "\n",
    "# Build idf scores for each word\n",
    "idf = s2v.build_idf(sentences)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print(\"\\nsentence embedding results without idf (BoV-mean)\\n\")\n",
    "print(\"sentence 10 is: \" + str(sentences[10]))\n",
    "print(\"The most similar sentences are: \")\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "print(\"\\nsentence 7 is: \" + str(sentences[7]))\n",
    "print(\"sentence 13 is: \" + str(sentences[13]))\n",
    "print(\"The score between sentence 7 and sentence 13 is: \")\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "print(\"\\nsentence embedding results with idf (BoV-idf)\\n\")\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6291M  100 6291M    0     0  5819k      0  0:18:27  0:18:27 --:--:-- 5804k804k      0  0:18:29  0:01:27  0:17:02 6041kM    0     0  5549k      0  0:19:20  0:02:03  0:17:17 2668k 0  4953k      0  0:21:40  0:03:53  0:17:47 4788kM    0     0  5004k      0  0:21:27  0:04:08  0:17:19 5856k5M    0     0  5117k      0  0:20:58  0:04:41  0:16:17 5943kM   32 2015M    0     0  5357k      0  0:20:02  0:06:25  0:13:37 5936k5476k      0  0:19:36  0:07:04  0:12:32 6016k  36 2281M    0     0  5483k      0  0:19:34  0:07:06  0:12:28 6353k 0     0  5506k      0  0:19:29  0:07:13  0:12:16 6814k 0  5641k      0  0:19:01  0:08:02  0:10:59 6836k 0:08:11  0:10:46 6815k   0     0  5681k      0  0:18:54  0:08:37  0:10:17 5902k1M    0     0  5694k      0  0:18:51  0:09:05  0:09:46 5971k91M   48 3043M    0     0  5694k      0  0:18:51  0:09:07  0:09:44 5806k 0:18:51  0:09:08  0:09:43 5881k06M    0     0  5699k      0  0:18:50  0:09:18  0:09:32 5923k91M   51 3248M    0     0  5704k      0  0:18:49  0:09:43  0:09:06 5644k 0  5708k      0  0:18:48  0:09:59  0:08:49 5913k622M    0     0  5732k      0  0:18:43  0:10:47  0:07:56 6149k 4054M    0     0  5756k      0  0:18:39  0:12:01  0:06:38 5910k  5757k      0  0:18:39  0:12:03  0:06:36 5843k  0  5762k      0  0:18:38  0:13:05  0:05:33 5927kM   72 4580M    0     0  5761k      0  0:18:38  0:13:34  0:05:04 5680k637M    0     0  5761k      0  0:18:38  0:13:44  0:04:54 5918k  84 5313M    0     0  5787k      0  0:18:33  0:15:40  0:02:53 5814k 0     0  5789k      0  0:18:32  0:15:50  0:02:42 6073k9M    0     0  5797k      0  0:18:31  0:16:43  0:01:48 5580k 0  0:18:30  0:16:53  0:01:37 6033k0  5807k      0  0:18:29  0:17:01  0:01:28 7071k  0     0  5812k      0  0:18:28  0:17:05  0:01:23 7158k 0  5816k      0  0:18:27  0:17:38  0:00:49 6136k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2886M  100 2886M    0     0  5898k      0  0:08:21  0:08:21 --:--:-- 5101k0  317M    0     0  5509k      0  0:08:56  0:00:58  0:07:58 6060k   14  411M    0     0  5613k      0  0:08:46  0:01:15  0:07:31 6021k 17  505M    0     0  5685k      0  0:08:39  0:01:30  0:07:09 6099k26  761M    0     0  5780k      0  0:08:31  0:02:14  0:06:17 5926k  0     0  5810k      0  0:08:28  0:02:34  0:05:54 5980k808k      0  0:08:28  0:02:37  0:05:51 5894k0  5778k      0  0:08:31  0:03:43  0:04:48 6140k  0  5872k      0  0:08:23  0:04:28  0:03:55 6524k 0  5878k      0  0:08:22  0:04:41  0:03:41 7204k72M    0     0  5906k      0  0:08:20  0:04:49  0:03:31 7232k   0  5922k      0  0:08:19  0:04:56  0:03:23 6422k  5938k      0  0:08:17  0:05:36  0:02:41 6227k 0  5939k      0  0:08:17  0:06:00  0:02:17 5913kM    0     0  5937k      0  0:08:17  0:06:12  0:02:05 5988k2M    0     0  5939k      0  0:08:17  0:06:17  0:02:00 6128k94k      0  0:08:21  0:06:42  0:01:39 5731k    0     0  5895k      0  0:08:21  0:06:48  0:01:33 5993k  0     0  5895k      0  0:08:21  0:06:51  0:01:30 5872k   0  5888k      0  0:08:22  0:06:59  0:01:23 5374k    0     0  5883k      0  0:08:22  0:07:02  0:01:20 5058k    0     0  5884k      0  0:08:22  0:07:03  0:01:19 5303k 0  5908k      0  0:08:20  0:07:43  0:00:37 5626k2 2676M    0     0  5907k      0  0:08:20  0:07:43  0:00:37 5592k2 2682M    0     0  5907k      0  0:08:20  0:07:44  0:00:36 5577kM    0     0  5907k      0  0:08:20  0:08:03  0:00:17 5811k\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "#     https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\n",
    "\n",
    "# TYPE CODE HERE\n",
    "!cd ./data/ && curl -O https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\n",
    "!cd ./data/ && curl -O https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 pretrained word vectors\n",
      "Loaded 50000 pretrained word vectors\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "w2v_en = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.en.vec'), nmax=50000)\n",
    "w2v_fr = Word2vec(os.path.join(PATH_TO_DATA, 'wiki.fr.vec'), nmax=50000)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for word in w2v_en.word2vec.keys():\n",
    "    if word in w2v_fr.word2vec.keys():\n",
    "        X.append(w2v_fr.word2vec[word])\n",
    "        Y.append(w2v_en.word2vec[word])\n",
    "        \n",
    "X = np.vstack(X).T\n",
    "Y = np.vstack(Y).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "# TYPE CODE HERE\n",
    "U, s, Vh = np.linalg.svd(np.dot(Y, X.T))\n",
    "W = np.dot(U, Vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English nearest neighbors for French word chat are: ['rabbit', 'hamster', 'feline', 'poodle', 'squirrel']\n",
      "The English nearest neighbors for French word chien are: ['poodle', 'terrier', 'dogs', 'spaniel', 'hamster']\n",
      "The English nearest neighbors for French word chiens are: ['rabbits', 'dog', 'hounds', 'hares', 'animals']\n",
      "The English nearest neighbors for French word paris are: ['parisian', 'rouen', 'gallimard', 'sorbonne', 'strasbourg']\n",
      "The English nearest neighbors for French word allemand are: ['austrian', 'prussian', 'luxembourgish', 'germany', 'friedrich']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "frs = ['chat', 'chien', 'chiens', 'paris', 'allemand']\n",
    "K = 5\n",
    "for fr in frs:\n",
    "    v_fr = w2v_fr.word2vec[fr]\n",
    "    v_en = np.dot(W, v_fr)\n",
    "    similarities = np.zeros(len(w2v_en.word2vec))\n",
    "    for idx, word in w2v_en.id2word.items():\n",
    "        v_word = w2v_en.word2vec[word]\n",
    "        similarities[idx] = w2v_en.cosine_similarity(v_en, v_word)\n",
    "\n",
    "    k_nearest_idxs = np.argsort(similarities)[-K-1:-1][::-1]\n",
    "\n",
    "    res = []\n",
    "    for idx in k_nearest_idxs:\n",
    "        res.append(w2v_en.id2word[idx])\n",
    "    \n",
    "    print(\"The English nearest neighbors for French word \" + fr + \" are: \" + str(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 labeled sentences\n",
      "Loaded 1101 labeled sentences\n",
      "Loaded 2210 test sentences\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def load_dataset(fname):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            label, sentence = line.split(' ', 1)\n",
    "            labels.append(int(label))\n",
    "            sentences.append(sentence)\n",
    "    print('Loaded %s labeled sentences' % (len(labels)))\n",
    "    return labels, sentences\n",
    "\n",
    "def load_test(fname):\n",
    "    sentences = []\n",
    "    with io.open(fname, encoding='utf-8') as f:\n",
    "        for _, line in enumerate(f):\n",
    "            sentences.append(line)\n",
    "    print('Loaded %s test sentences' % (len(sentences)))\n",
    "    return sentences\n",
    "\n",
    "labels_train, sentences_train = load_dataset(PATH_TO_DATA + 'SST/stsa.fine.train')\n",
    "labels_dev, sentences_dev = load_dataset(PATH_TO_DATA + 'SST/stsa.fine.dev')\n",
    "sentences_test = load_test(PATH_TO_DATA + 'SST/stsa.fine.test.X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "\n",
    "# TYPE CODE HERE\n",
    "def encode_dataset(s2v, sentences, idf=False):\n",
    "    sentemb = s2v.encode(sentences, idf)\n",
    "    return sentemb\n",
    "\n",
    "# without idf (BoV-mean)\n",
    "sentemb_train = encode_dataset(s2v, sentences_train)\n",
    "sentemb_dev = encode_dataset(s2v, sentences_dev)\n",
    "sentemb_test = encode_dataset(s2v, sentences_test)\n",
    "\n",
    "# with idf (BoV-idf)\n",
    "idf = s2v.build_idf(sentences_train)\n",
    "sentemb_train_idf = encode_dataset(s2v, sentences_train, idf)\n",
    "sentemb_dev_idf = encode_dataset(s2v, sentences_dev, idf)\n",
    "sentemb_test_idf = encode_dataset(s2v, sentences_test, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without idf\n",
      "\n",
      "Accuracy on the training set: 0.49953183520599254 (C = 5.0)\n",
      "Accuracy on the dev set: 0.40054495912806537\n",
      "Accuracy on the training set: 0.4887640449438202 (C = 1.0)\n",
      "Accuracy on the dev set: 0.40962761126248864\n",
      "Accuracy on the training set: 0.4820926966292135 (C = 0.5)\n",
      "Accuracy on the dev set: 0.4141689373297003\n",
      "Accuracy on the training set: 0.455875468164794 (C = 0.1)\n",
      "Accuracy on the dev set: 0.4223433242506812\n",
      "Accuracy on the training set: 0.4417134831460674 (C = 0.05)\n",
      "Accuracy on the dev set: 0.4305177111716621\n",
      "Accuracy on the training set: 0.4187734082397004 (C = 0.02)\n",
      "Accuracy on the dev set: 0.4087193460490463\n",
      "Accuracy on the training set: 0.4044943820224719 (C = 0.01)\n",
      "Accuracy on the dev set: 0.3851044504995459\n",
      "\n",
      "Results with idf\n",
      "\n",
      "Accuracy on the training set: 0.4940308988764045 (C = 5.0)\n",
      "Accuracy on the dev set: 0.39509536784741145\n",
      "Accuracy on the training set: 0.4943820224719101 (C = 1.0)\n",
      "Accuracy on the dev set: 0.3887375113533152\n",
      "Accuracy on the training set: 0.49122191011235955 (C = 0.5)\n",
      "Accuracy on the dev set: 0.4014532243415077\n",
      "Accuracy on the training set: 0.4820926966292135 (C = 0.1)\n",
      "Accuracy on the dev set: 0.4123524069028156\n",
      "Accuracy on the training set: 0.47425093632958804 (C = 0.05)\n",
      "Accuracy on the dev set: 0.4196185286103542\n",
      "Accuracy on the training set: 0.460440074906367 (C = 0.02)\n",
      "Accuracy on the dev set: 0.41689373297002724\n",
      "Accuracy on the training set: 0.44662921348314605 (C = 0.01)\n",
      "Accuracy on the dev set: 0.39509536784741145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=500, multi_class='multinomial',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# tune L2 regularization on the dev set\n",
    "Cs = [5.0, 1.0, 0.5, 0.1, 0.05, 0.02, 0.01]\n",
    "# without idf\n",
    "print(\"Results without idf\\n\")\n",
    "for c in Cs:\n",
    "    logreg = sklearn.linear_model.LogisticRegression(C=c, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
    "    logreg.fit(sentemb_train, labels_train)\n",
    "    accuracy_train = logreg.score(sentemb_train, labels_train)\n",
    "    print(\"Accuracy on the training set: \" + str(accuracy_train) + \" (C = \" + str(c) + \")\")\n",
    "    accuracy_dev = logreg.score(sentemb_dev_idf, labels_dev)\n",
    "    print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# with idf\n",
    "print(\"\\nResults with idf\\n\")\n",
    "for c in Cs:\n",
    "    logreg = sklearn.linear_model.LogisticRegression(C=c, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
    "    logreg.fit(sentemb_train_idf, labels_train)\n",
    "    accuracy_train = logreg.score(sentemb_train_idf, labels_train)\n",
    "    print(\"Accuracy on the training set: \" + str(accuracy_train) + \" (C = \" + str(c) + \")\")\n",
    "    accuracy_dev = logreg.score(sentemb_dev_idf, labels_dev)\n",
    "    print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "    \n",
    "# choose C = 0.05 without idf 'lbfgs' solver and 'multinomial', max_iter = 500 to train on total training set\n",
    "sentemb_total = np.concatenate((sentemb_train, sentemb_dev), axis=0)\n",
    "labels_total = labels_train + labels_dev\n",
    "logreg = sklearn.linear_model.LogisticRegression(C=0.05, solver='lbfgs', multi_class='multinomial', max_iter=500)\n",
    "logreg.fit(sentemb_total, labels_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "y_pred = logreg.predict(sentemb_test_idf)\n",
    "pd.DataFrame(y_pred).to_csv('logreg_bov_y_test_sst.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried several classifiers (e.g. SVM, Random Forest, Gradient boosting), however these classifiers do not give a better performance than logistic regression. This may because that we use aggregated features which is too simple, in which case simple classifier gives better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results without idf\n",
      "\n",
      "Accuracy on the training set: 0.5912921348314607\n",
      "Accuracy on the dev set: 0.4032697547683924\n",
      "\n",
      "Results with idf\n",
      "\n",
      "Accuracy on the training set: 0.5870786516853933\n",
      "Accuracy on the dev set: 0.3832879200726612\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "# # try SVM classifier\n",
    "# Cs = [5.0, 1.0, 0.5, 0.1, 0.05, 0.02, 0.01]\n",
    "# # without idf\n",
    "# print(\"Results without idf\\n\")\n",
    "# for c in Cs:\n",
    "#     svm = sklearn.svm.SVC(C=c, decision_function_shape='ovo')\n",
    "#     svm.fit(sentemb_train, labels_train)\n",
    "#     accuracy_train = svm.score(sentemb_train, labels_train)\n",
    "#     print(\"Accuracy on the training set: \" + str(accuracy_train) + \" (C = \" + str(c) + \")\")\n",
    "#     accuracy_dev = svm.score(sentemb_dev, labels_dev)\n",
    "#     print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# # with idf\n",
    "# print(\"\\nResults with idf\\n\")\n",
    "# for c in Cs:\n",
    "#     svm = sklearn.svm.SVC(C=c, decision_function_shape='ovo')\n",
    "#     svm.fit(sentemb_train_idf, labels_train)\n",
    "#     accuracy_train = svm.score(sentemb_train_idf, labels_train)\n",
    "#     print(\"Accuracy on the training set: \" + str(accuracy_train) + \" (C = \" + str(c) + \")\")\n",
    "#     accuracy_dev = svm.score(sentemb_dev_idf, labels_dev)\n",
    "#     print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# # try Random Forest classifier\n",
    "# # without idf\n",
    "# print(\"Results without idf\\n\")\n",
    "# rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=6)\n",
    "# rf.fit(sentemb_train, labels_train)\n",
    "# accuracy_train = rf.score(sentemb_train, labels_train)\n",
    "# print(\"Accuracy on the training set: \" + str(accuracy_train))\n",
    "# accuracy_dev = rf.score(sentemb_dev, labels_dev)\n",
    "# print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# # with idf\n",
    "# print(\"\\nResults with idf\\n\")\n",
    "# rf = sklearn.ensemble.RandomForestClassifier(n_estimators=100, max_depth=6)\n",
    "# rf.fit(sentemb_train_idf, labels_train)\n",
    "# accuracy_train = rf.score(sentemb_train_idf, labels_train)\n",
    "# print(\"Accuracy on the training set: \" + str(accuracy_train))\n",
    "# accuracy_dev = rf.score(sentemb_dev_idf, labels_dev)\n",
    "# print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# try gradient boosting classifier\n",
    "# without idf\n",
    "print(\"Results without idf\\n\")\n",
    "gdbt = sklearn.ensemble.GradientBoostingClassifier(learning_rate=0.05, n_estimators=100, max_depth=3)\n",
    "gdbt.fit(sentemb_train, labels_train)\n",
    "accuracy_train = gdbt.score(sentemb_train, labels_train)\n",
    "print(\"Accuracy on the training set: \" + str(accuracy_train))\n",
    "accuracy_dev = gdbt.score(sentemb_dev, labels_dev)\n",
    "print(\"Accuracy on the dev set: \" + str(accuracy_dev))\n",
    "\n",
    "# with idf\n",
    "print(\"\\nResults with idf\\n\")\n",
    "gdbt = sklearn.ensemble.GradientBoostingClassifier(learning_rate=0.05, n_estimators=100, max_depth=3)\n",
    "gdbt.fit(sentemb_train_idf, labels_train)\n",
    "accuracy_train = gdbt.score(sentemb_train_idf, labels_train)\n",
    "print(\"Accuracy on the training set: \" + str(accuracy_train))\n",
    "accuracy_dev = gdbt.score(sentemb_dev_idf, labels_dev)\n",
    "print(\"Accuracy on the dev set: \" + str(accuracy_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8544 labeled sentences\n",
      "Loaded 1101 labeled sentences\n",
      "Loaded 2210 test sentences\n"
     ]
    }
   ],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "PATH_TO_DATA = \"./data/\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "labels_train, sentences_train = load_dataset(PATH_TO_DATA + 'SST/stsa.fine.train')\n",
    "labels_dev, sentences_dev = load_dataset(PATH_TO_DATA + 'SST/stsa.fine.dev')\n",
    "sentences_test = load_test(PATH_TO_DATA + 'SST/stsa.fine.test.X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16483\n",
      "18131\n"
     ]
    }
   ],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.text import text_to_word_sequence, one_hot\n",
    "def getTotalText(sents):\n",
    "    text = \"\"\n",
    "    for sent in sents:\n",
    "        for s in sent:\n",
    "            text += s.strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "text = getTotalText([sentences_train, sentences_dev])\n",
    "\n",
    "vocabs = text_to_word_sequence(text)\n",
    "# print(vocabs[:100])\n",
    "vocab_size = len(set(vocabs))\n",
    "print(vocab_size)\n",
    "\n",
    "extended_vocab_size = round(vocab_size * 1.1)\n",
    "print(extended_vocab_size)\n",
    "\n",
    "def oneHotEncoding(sentences, n):\n",
    "    encodeds = []\n",
    "    for sent in sentences:\n",
    "        encoded = one_hot(sent, n)\n",
    "        encodeds.append(encoded)\n",
    "        \n",
    "    return encodeds\n",
    "\n",
    "encoded_train = oneHotEncoding(sentences_train, extended_vocab_size)\n",
    "encoded_dev = oneHotEncoding(sentences_dev, extended_vocab_size)\n",
    "encoded_test = oneHotEncoding(sentences_test, extended_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train = pad_sequences(encoded_train, padding='post', dtype='int32')\n",
    "X_dev = pad_sequences(encoded_dev, padding='post', dtype='int32')\n",
    "X_test = pad_sequences(encoded_test, padding='post', dtype='int32')\n",
    "\n",
    "y_train = to_categorical(labels_train)\n",
    "y_dev = to_categorical(labels_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation, Dropout\n",
    "from keras import regularizers\n",
    "\n",
    "embed_dim  = 300  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = extended_vocab_size  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(n_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, None, 300)         5439300   \n",
      "_________________________________________________________________\n",
      "lstm_30 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 5,533,065\n",
      "Trainable params: 5,533,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  Adam(lr=0.01, beta_1=0.5, beta_2=0.999, epsilon=1e-8, decay=1e-6) # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/20\n",
      "8544/8544 [==============================] - 51s 6ms/step - loss: 1.5758 - acc: 0.2693 - val_loss: 1.5723 - val_acc: 0.2525\n",
      "Epoch 2/20\n",
      "8544/8544 [==============================] - 45s 5ms/step - loss: 1.5715 - acc: 0.2666 - val_loss: 1.5729 - val_acc: 0.2525\n",
      "Epoch 3/20\n",
      "8544/8544 [==============================] - 45s 5ms/step - loss: 1.5687 - acc: 0.2677 - val_loss: 1.5769 - val_acc: 0.2525\n",
      "Epoch 4/20\n",
      "8544/8544 [==============================] - 45s 5ms/step - loss: 1.5693 - acc: 0.2656 - val_loss: 1.5777 - val_acc: 0.2525\n",
      "Epoch 5/20\n",
      "8544/8544 [==============================] - 45s 5ms/step - loss: 1.5698 - acc: 0.2687 - val_loss: 1.5786 - val_acc: 0.2525\n",
      "Epoch 6/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.5696 - acc: 0.2699 - val_loss: 1.5866 - val_acc: 0.2380\n",
      "Epoch 7/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.5707 - acc: 0.2714 - val_loss: 1.5870 - val_acc: 0.2525\n",
      "Epoch 8/20\n",
      "8544/8544 [==============================] - 45s 5ms/step - loss: 1.5608 - acc: 0.2678 - val_loss: 1.5819 - val_acc: 0.2579\n",
      "Epoch 9/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.5475 - acc: 0.2793 - val_loss: 1.5839 - val_acc: 0.2625\n",
      "Epoch 10/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.5226 - acc: 0.3125 - val_loss: 1.5993 - val_acc: 0.2661\n",
      "Epoch 11/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.4987 - acc: 0.3310 - val_loss: 1.6157 - val_acc: 0.2570\n",
      "Epoch 12/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.4787 - acc: 0.3546 - val_loss: 1.6154 - val_acc: 0.2670\n",
      "Epoch 13/20\n",
      "8544/8544 [==============================] - 6958s 814ms/step - loss: 1.4533 - acc: 0.3722 - val_loss: 1.6454 - val_acc: 0.2461\n",
      "Epoch 14/20\n",
      "8544/8544 [==============================] - 66s 8ms/step - loss: 1.4410 - acc: 0.3791 - val_loss: 1.6375 - val_acc: 0.2598\n",
      "Epoch 15/20\n",
      "8544/8544 [==============================] - 50s 6ms/step - loss: 1.4211 - acc: 0.3955 - val_loss: 1.6617 - val_acc: 0.2634\n",
      "Epoch 16/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.4023 - acc: 0.4045 - val_loss: 1.6765 - val_acc: 0.2679\n",
      "Epoch 17/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.3910 - acc: 0.4135 - val_loss: 1.6849 - val_acc: 0.2480\n",
      "Epoch 18/20\n",
      "8544/8544 [==============================] - 47s 5ms/step - loss: 1.3745 - acc: 0.4211 - val_loss: 1.6931 - val_acc: 0.2743\n",
      "Epoch 19/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.3681 - acc: 0.4288 - val_loss: 1.7022 - val_acc: 0.2616\n",
      "Epoch 20/20\n",
      "8544/8544 [==============================] - 46s 5ms/step - loss: 1.3542 - acc: 0.4422 - val_loss: 1.7076 - val_acc: 0.2661\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 32\n",
    "n_epochs = 20\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "pd.DataFrame(y_pred).to_csv('logreg_lstm_y_test_sst.txt', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_acc', 'loss', 'val_loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x189e83b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17161a898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, None, 300)         5439300   \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, None, 300)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, None, 64)          57664     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_15 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 5,497,289\n",
      "Trainable params: 5,497,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, GlobalMaxPooling1D\n",
    "\n",
    "# get pretrained embedding weights\n",
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "word_embed_dim = 300\n",
    "dist_vocabs = set(vocabs)\n",
    "embeddings = np.zeros((extended_vocab_size, word_embed_dim))\n",
    "unseen = 0\n",
    "for i, c in enumerate(dist_vocabs):\n",
    "    encode = one_hot(c, extended_vocab_size)\n",
    "    if c in w2v.word2vec.keys():\n",
    "        embeddings[encode] = w2v.word2vec[c]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embeddings.shape[0], \n",
    "                      output_dim=word_embed_dim,\n",
    "                      weights=[embeddings], # we pass our pre-trained embeddings\n",
    "                      trainable=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, kernel_regularizer=regularizers.l2(0.01), padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=1e-6)\n",
    "\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/5\n",
      "8544/8544 [==============================] - 23s 3ms/step - loss: 2.2488 - acc: 0.2844 - val_loss: 1.8660 - val_acc: 0.3606\n",
      "Epoch 2/5\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.7089 - acc: 0.3826 - val_loss: 1.5958 - val_acc: 0.3878\n",
      "Epoch 3/5\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.5065 - acc: 0.4325 - val_loss: 1.4900 - val_acc: 0.3987\n",
      "Epoch 4/5\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.4023 - acc: 0.4652 - val_loss: 1.4625 - val_acc: 0.4114\n",
      "Epoch 5/5\n",
      "8544/8544 [==============================] - 19s 2ms/step - loss: 1.3212 - acc: 0.5119 - val_loss: 1.4542 - val_acc: 0.4142\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "n_epochs = 5\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "pd.DataFrame(y_pred).to_csv('pretrained_conv1d_y_test_sst.txt', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
